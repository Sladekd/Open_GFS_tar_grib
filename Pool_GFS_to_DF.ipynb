{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pygrib\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Replace this with the actual path to your tar files directory\n",
        "tar_files_directory = '/content/'\n",
        "temp_dir = '/content/temp_dir/'\n",
        "\n",
        "# Function to process a single .grib file\n",
        "def process_grib_file(grib_file):\n",
        "    #print(f\"Processing {grib_file}...\")  # Debugging: Check which file is being processed\n",
        "\n",
        "    data = {\n",
        "        'lat': [],\n",
        "        'lon': [],\n",
        "        'level': [],\n",
        "        'param_name': [],\n",
        "        'param_value': [],\n",
        "        'forecast_step': [],\n",
        "        'Date': [],\n",
        "        'time': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with pygrib.open(grib_file) as grbs:\n",
        "            for grb in grbs:\n",
        "                latitudes, longitudes, values = grb.latitudes, grb.longitudes, grb.values\n",
        "                data['lat'].extend(latitudes.ravel())\n",
        "                data['lon'].extend(longitudes.ravel())\n",
        "                data['level'].extend([grb['level']] * latitudes.size)\n",
        "                data['param_name'].extend([grb['name']] * latitudes.size)\n",
        "                data['param_value'].extend(values.ravel())\n",
        "                data['forecast_step'].extend([grb['step']] * latitudes.size)\n",
        "                data['Date'].extend([grb.validityDate] * latitudes.size)\n",
        "                data['time'].extend([grb.validityTime] * latitudes.size)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {grib_file}: {e}\")  # Debugging: Catch and print any error\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to extract .tar files and process the .grib files\n",
        "def process_tar_file(tar_file_path):\n",
        "    print(f\"Processing TAR file: {tar_file_path}\")  # Debugging: Check which tar file is being processed\n",
        "\n",
        "    # Extract files from tar archive\n",
        "    with tarfile.open(tar_file_path, 'r') as tar:\n",
        "        tar.extractall(path=temp_dir)\n",
        "\n",
        "    # Get all grib files extracted from the tar\n",
        "    grib_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.grib2')]\n",
        "    print(f\"Found {len(grib_files)} .grib files in {tar_file_path}\")  # Debugging: Check if grib files were found\n",
        "\n",
        "    if not grib_files:\n",
        "        print(f\"No .grib files found in {tar_file_path}\")  # Debugging: Alert if no grib files\n",
        "\n",
        "    # Process each grib file in parallel\n",
        "    with Pool() as pool:\n",
        "        result_data = pool.map(process_grib_file, grib_files)\n",
        "\n",
        "    # Clear the temporary directory after processing\n",
        "    for file in os.listdir(temp_dir):\n",
        "        os.remove(os.path.join(temp_dir, file))\n",
        "\n",
        "    # Combine all data into a single dictionary, check if result_data is not empty\n",
        "    if not result_data:\n",
        "        print(f\"No data processed for {tar_file_path}\")  # Debugging: Alert if no data is processed\n",
        "\n",
        "    combined_data = {key: [] for key in result_data[0].keys()} if result_data else {}\n",
        "    for data_chunk in result_data:\n",
        "        for key in combined_data:\n",
        "            combined_data[key].extend(data_chunk[key])\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# Main execution loop\n",
        "def process_all_tar_files(tar_files_directory):\n",
        "    all_data = {\n",
        "        'lat': [],\n",
        "        'lon': [],\n",
        "        'level': [],\n",
        "        'param_name': [],\n",
        "        'param_value': [],\n",
        "        'forecast_step': [],\n",
        "        'Date': [],\n",
        "        'time': []\n",
        "    }\n",
        "\n",
        "    tar_files = [os.path.join(tar_files_directory, file) for file in os.listdir(tar_files_directory) if file.endswith('.tar')]\n",
        "\n",
        "    for tar_file in tar_files:\n",
        "        file_data = process_tar_file(tar_file)\n",
        "\n",
        "        for key in all_data:\n",
        "            all_data[key].extend(file_data.get(key, []))  # Use .get to avoid key errors\n",
        "\n",
        "    # Convert the data dictionary to a pandas DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Run the process\n",
        "df = process_all_tar_files(tar_files_directory)\n"
      ],
      "metadata": {
        "id": "B6zuC4ekvyPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}