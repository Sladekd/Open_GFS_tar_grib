{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnja0GCFK2DW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This notebook works for processing GRIB files of GFS from the NCAR archive.\n",
        "It uses pygrib library, but you can use xarray as well. It also provides xr.to_dataframe method, which is very straightforward but can be time consuming.\n",
        "\n",
        "This is supposed to be used in google Colaboratory, where pygrib works somehow better.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If needed:\n",
        "#!pip install pygrib"
      ],
      "metadata": {
        "id": "wLrbShE_M01L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean folder you are about to use\n",
        "import os, shutil\n",
        "\n",
        "def clean_content(folder):\n",
        "\n",
        "  for filename in os.listdir(folder):\n",
        "    file_path = os.path.join(folder, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
      ],
      "metadata": {
        "id": "k6ZYk-RVQpeY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_content('/content/temp_dir/')"
      ],
      "metadata": {
        "id": "iwZGqn3zQ6YU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted code with latitude and longitude information\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import pygrib\n",
        "\n",
        "# Replace this with the actual path to your tar files directory\n",
        "tar_files_directory = '/content/'\n",
        "# Temporary directory to extract files\n",
        "temp_dir = '/content/temp_dir/'\n",
        "\n",
        "# Dictionary to store extracted information\n",
        "data = {\n",
        "    'level': [],\n",
        "    'param_name': [],\n",
        "    'param_value': [],\n",
        "    'filename': [],\n",
        "    'forecast_step': [],\n",
        "    'Date': [],\n",
        "    'time': [],\n",
        "    'latitude': [],  # Add latitude\n",
        "    'longitude': []  # Add longitude\n",
        "}\n",
        "\n",
        "# Iterate through each tar file in the directory\n",
        "for tar_file in os.listdir(tar_files_directory):\n",
        "    if tar_file.endswith(\".tar\"):\n",
        "        tar_file_path = os.path.join(tar_files_directory, tar_file)\n",
        "\n",
        "        # Step 1: Extract all files from the tar archive into the temporary directory\n",
        "        with tarfile.open(tar_file_path, 'r') as tar:\n",
        "            tar.extractall(path=temp_dir)\n",
        "\n",
        "        # Step 2: Use pygrib.open to open each grib file and extract information\n",
        "        grib_files = [os.path.join(temp_dir, file) for file in os.listdir(temp_dir)]\n",
        "\n",
        "        for grib_file in grib_files:\n",
        "            with pygrib.open(grib_file) as grbs:\n",
        "                for grb in grbs:\n",
        "                    data['level'].append(grb['level'])\n",
        "                    data['param_name'].append(grb['name'])\n",
        "                    data['param_value'].append(grb.values)\n",
        "                    data['filename'].append(os.path.basename(tar_file))\n",
        "                    data['forecast_step'].append(grb['step'])\n",
        "                    data['Date'].append(grb.validityDate)\n",
        "                    data['time'].append(grb.validityTime)\n",
        "                    data['latitude'].append(grb['latitudes'])\n",
        "                    data['longitude'].append(grb['longitudes'])\n",
        "\n",
        "        # Clear the temporary directory\n",
        "        for file in os.listdir(temp_dir):\n",
        "            file_path = os.path.join(temp_dir, file)\n",
        "            os.remove(file_path)\n",
        "\n",
        "# Step 4: Concatenate all the data into one DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the concatenated DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "pZ3OqjE9Pn4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There is time in the format 0, 600, 1800, this cell fixes the format and creates datetime Column\n",
        "\n",
        "df['time'] = pd.to_numeric(df['time'], errors='coerce')\n",
        "\n",
        "df['time'] = df['time'].astype(str).str.zfill(4)\n",
        "\n",
        "# Concatenate 'valid' and 'time' to create a new datetime column\n",
        "\n",
        "df['datetime_str'] = df['Date'].astype(str) + df['time'].astype(str).str.zfill(4)\n",
        "# Convert the concatenated string to datetime\n",
        "df['datetime'] = pd.to_datetime(df['datetime_str'], format='%Y%m%d%H%M')\n"
      ],
      "metadata": {
        "id": "Vz_SllN_M6rW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
